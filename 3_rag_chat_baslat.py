import gradio as gr
import torch
from transformers import pipeline
from langchain_community.llms import HuggingFacePipeline
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS
from langchain.chains import RetrievalQA
from langchain.prompts import PromptTemplate
from langchain_community.document_loaders import TextLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
import os

# --- 1. AYARLAR VE MODEL ---
print("ğŸš€ Sistem baÅŸlatÄ±lÄ±yor... (TÃ¼rkÃ§e Zorlama Modu v3.0)")

model_id = "TinyLlama/TinyLlama-1.1B-Chat-v1.0"

pipe = pipeline(
    "text-generation",
    model=model_id,
    torch_dtype=torch.float32,
    device_map="auto",
    max_new_tokens=256,
    do_sample=True,
    temperature=0.2,          # DÃ¼ÅŸÃ¼k sÄ±caklÄ±k (YaratÄ±cÄ±lÄ±ÄŸÄ± kÄ±sÄ±tla)
    top_p=0.90,
    repetition_penalty=1.1    # Tekrar cezasÄ±nÄ± biraz azalttÄ±k (Ã‡ok yÃ¼ksek olunca dil bozulabiliyor)
)
llm = HuggingFacePipeline(pipeline=pipe)

embedding_model = HuggingFaceEmbeddings(model_name="sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2")

# --- 2. HAFIZA ---
print("ğŸ“š HafÄ±za kontrol ediliyor...")
# Veriyi her seferinde tazelemek en garantisi
loader = TextLoader("alzheimer_veri.txt", encoding="utf-8")
docs = loader.load()

text_splitter = RecursiveCharacterTextSplitter(chunk_size=512, chunk_overlap=50)
parcalar = text_splitter.split_documents(docs)

vector_store = FAISS.from_documents(parcalar, embedding_model)
print("âœ… HafÄ±za hazÄ±r!")

# --- 3. PROMPT (Ã‡OK KATI TÃœRKÃ‡E KURALLARI) ---
# Ä°ngilizce konuÅŸmasÄ±nÄ± yasaklayan ve cevabÄ± doÄŸrudan veriden Ã§ekmesini saÄŸlayan ÅŸablon
template = """<|system|>
Sen TÃ¼rkÃ§e konuÅŸan uzman bir asistansÄ±n.
SANA VERÄ°LEN BAÄLAMDAKÄ° BÄ°LGÄ°LERÄ° KULLANARAK CEVAP VER.
Kendi bilgini katma. Sadece TÃœRKÃ‡E cevap ver. Ä°ngilizce konuÅŸma.

BaÄŸlam:
{context}
</s>
<|user|>
Soru: {question}
</s>
<|assistant|>
Cevap:"""  # Cevap: diyerek baÅŸlamaya zorluyoruz

PROMPT = PromptTemplate(template=template, input_variables=["context", "question"])

qa_chain = RetrievalQA.from_chain_type(
    llm=llm,
    chain_type="stuff",
    retriever=vector_store.as_retriever(search_kwargs={"k": 2}),
    chain_type_kwargs={"prompt": PROMPT}
)

# --- 4. TEMÄ°ZLÄ°K VE ZORLAMA FONKSÄ°YONU ---
def cevapla(soru):
    if not soru:
        return ""
    
    # Modele soruyu sor
    ham_cevap = qa_chain.invoke({"query": soru})
    metin = ham_cevap["result"]
    
    # --- TEMÄ°ZLÄ°K ANI ---
    # Modelin Ã¼rettiÄŸi cevabÄ±n iÃ§inden sadece gerekli kÄ±smÄ± al
    if "<|assistant|>" in metin:
        temiz_cevap = metin.split("<|assistant|>")[-1]
    else:
        temiz_cevap = metin
        
    # EÄŸer "Cevap:" kelimesi varsa ondan sonrasÄ±nÄ± al
    if "Cevap:" in temiz_cevap:
        temiz_cevap = temiz_cevap.split("Cevap:")[-1]

    # HÃ¢lÃ¢ Ä°ngilizce "Sure!" veya "Here is..." gibi kalÄ±plar varsa temizle (Basit filtre)
    yasakli_kelimeler = ["Sure", "Here is", "In this case", "Context:", "Question:"]
    for kelime in yasakli_kelimeler:
        temiz_cevap = temiz_cevap.replace(kelime, "")

    return temiz_cevap.strip()

# --- 5. ARAYÃœZ ---
arayuz = gr.Interface(
    fn=cevapla,
    inputs=gr.Textbox(lines=2, placeholder="Ã–rn: Annem banyo yapmak istemiyor, ne yapmalÄ±yÄ±m?"),
    outputs=gr.Textbox(label="Uzman CevabÄ±"),
    title="ğŸ§  Alzheimer AsistanÄ± (TÃ¼rkÃ§e v3.0)",
    description="Akademik ve pratik bakÄ±m rehberiniz. Sadece TÃ¼rkÃ§e cevap verir."
)

if __name__ == "__main__":
    arayuz.launch()
