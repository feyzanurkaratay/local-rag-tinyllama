import gradio as gr
import torch
from transformers import pipeline
from langchain_community.llms import HuggingFacePipeline
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS
from langchain.chains import RetrievalQA
from langchain.prompts import PromptTemplate
from langchain_community.document_loaders import TextLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
import os
import sys

# --- 1. MODEL AYARLARI ---
print("ğŸš€ Sistem BaÅŸlatÄ±lÄ±yor... (DÄ°KTATÃ–R MODU)")

model_id = "TinyLlama/TinyLlama-1.1B-Chat-v1.0"

pipe = pipeline(
    "text-generation",
    model=model_id,
    torch_dtype=torch.float32,
    device_map="auto",
    max_new_tokens=256,
    do_sample=True,
    temperature=0.1,          # YaratÄ±cÄ±lÄ±k KAPALI. Sadece okuduÄŸunu sÃ¶yler.
    top_p=0.90,
    repetition_penalty=1.2    # Tekrar etmeyi engeller.
)
llm = HuggingFacePipeline(pipeline=pipe)

# --- 2. HAFIZA YÃœKLEME ---
print("ğŸ“š HafÄ±za yÃ¼kleniyor...")
if not os.path.exists("alzheimer_veri.txt"):
    print("âŒ HATA: Veri dosyasÄ± yok! Ã–nce 1_veri_olustur.py Ã§alÄ±ÅŸtÄ±r.")
    sys.exit()

loader = TextLoader("alzheimer_veri.txt", encoding="utf-8")
docs = loader.load()

# Chunk'larÄ± bÃ¼yÃ¼ttÃ¼k (500) ki konu bÃ¼tÃ¼nlÃ¼ÄŸÃ¼ bozulmasÄ±n
text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)
parcalar = text_splitter.split_documents(docs)

embedding_model = HuggingFaceEmbeddings(model_name="sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2")
vector_store = FAISS.from_documents(parcalar, embedding_model)
print("âœ… HafÄ±za hazÄ±r!")

# --- 3. SERT PROMPT (YORUM YOK, SADECE OKU) ---
template = """<|system|>
You are a strict assistant. 
Read the Turkish CONTEXT below.
Answer the QUESTION using ONLY the CONTEXT.
If the answer is not in the context, say "Bilmiyorum".
Answer in TURKISH.

CONTEXT:
{context}
</s>
<|user|>
QUESTION: {question}
</s>
<|assistant|>
"""

PROMPT = PromptTemplate(template=template, input_variables=["context", "question"])

qa_chain = RetrievalQA.from_chain_type(
    llm=llm,
    chain_type="stuff",
    retriever=vector_store.as_retriever(search_kwargs={"k": 2}),
    chain_type_kwargs={"prompt": PROMPT}
)

# --- 4. CEVAP TEMÄ°ZLEME ---
def cevapla(soru):
    if not soru:
        return ""
    
    ham_cevap = qa_chain.invoke({"query": soru})
    metin = ham_cevap["result"]
    
    # Modelin teknik etiketlerini temizle
    if "<|assistant|>" in metin:
        temiz_cevap = metin.split("<|assistant|>")[-1]
    else:
        temiz_cevap = metin

    # EÄŸer Ä°ngilizce baÅŸlarsa uyar
    if "Sure!" in temiz_cevap or "Here is" in temiz_cevap:
        return "âš ï¸ Model Ä°ngilizceye kaÃ§tÄ±. LÃ¼tfen soruyu 'Araba kullanabilir mi?' ÅŸeklinde net sorun."
        
    return temiz_cevap.strip()

# --- 5. ARAYÃœZ ---
arayuz = gr.Interface(
    fn=cevapla,
    inputs=gr.Textbox(lines=2, placeholder="Ã–rn: Araba kullanabilir mi?"),
    outputs=gr.Textbox(label="Cevap"),
    title="ğŸ§  Alzheimer AsistanÄ± (SÄ±kÄ± YÃ¶netim)",
    description="Sadece veri tabanÄ±ndaki doÄŸru bilgileri verir. Uydurmaz."
)

if __name__ == "__main__":
    arayuz.launch(inbrowser=True)
