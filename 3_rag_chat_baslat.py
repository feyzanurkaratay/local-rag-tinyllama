import gradio as gr
import torch
from transformers import pipeline
from langchain_community.llms import HuggingFacePipeline
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS
from langchain.chains import RetrievalQA
from langchain.prompts import PromptTemplate
from langchain_community.document_loaders import TextLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
import os
import sys

# --- 1. AYARLAR ---
print("ğŸš€ Sistem BaÅŸlatÄ±lÄ±yor... (Genel Uzman Modu)")

model_id = "TinyLlama/TinyLlama-1.1B-Chat-v1.0"

pipe = pipeline(
    "text-generation",
    model=model_id,
    torch_dtype=torch.float32,
    device_map="auto",
    max_new_tokens=512,       # Daha uzun cevaplar verebilsin
    do_sample=True,
    temperature=0.4,          # YaratÄ±cÄ±lÄ±ÄŸÄ± artÄ±rdÄ±k (Daha doÄŸal konuÅŸsun)
    top_p=0.92,
    repetition_penalty=1.1
)
llm = HuggingFacePipeline(pipeline=pipe)

# --- 2. HAFIZA ---
print("ğŸ“š HafÄ±za yÃ¼kleniyor...")
# Veri dosyasÄ± varsa yÃ¼kle, yoksa hata verme (Sadece genel bilgiyle Ã§alÄ±ÅŸabilsin diye)
vector_store = None
if os.path.exists("alzheimer_veri.txt"):
    embedding_model = HuggingFaceEmbeddings(model_name="sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2")
    loader = TextLoader("alzheimer_veri.txt", encoding="utf-8")
    docs = loader.load()
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=400, chunk_overlap=40)
    parcalar = text_splitter.split_documents(docs)
    vector_store = FAISS.from_documents(parcalar, embedding_model)
    print("âœ… Yerel veri kaynaÄŸÄ± (RAG) yÃ¼klendi.")
else:
    print("âš ï¸ UYARI: Veri dosyasÄ± bulunamadÄ±. Model sadece genel bilgisiyle cevap verecek.")

# --- 3. HÄ°BRÄ°T PROMPT (KÄ°LÄ°T NOKTA) ---
# Modele diyoruz ki: Ã–nce elindeki nota bak, orada yoksa bildiÄŸin gibi anlat.
template = """<|system|>
Sen Alzheimer konusunda uzman, yardÄ±msever bir asistansÄ±n.
Sana bir BAÄLAM (Context) verilecek. 
Ã–nce bu baÄŸlamdaki bilgileri kullan. EÄŸer sorunun cevabÄ± baÄŸlamda yoksa, KENDÄ° GENEL BÄ°LGÄ°NÄ° kullanarak cevapla.
Her zaman TÃœRKÃ‡E cevap ver.

BAÄLAM:
{context}
</s>
<|user|>
SORU: {question}
</s>
<|assistant|>
"""

PROMPT = PromptTemplate(template=template, input_variables=["context", "question"])

# --- 4. ZÄ°NCÄ°RÄ° KUR ---
if vector_store:
    retriever = vector_store.as_retriever(search_kwargs={"k": 2})
    qa_chain = RetrievalQA.from_chain_type(
        llm=llm,
        chain_type="stuff",
        retriever=retriever,
        chain_type_kwargs={"prompt": PROMPT}
    )
else:
    # EÄŸer veritabanÄ± yoksa dÃ¼z LLM zinciri (Fallback)
    qa_chain = None 

# --- 5. CEVAP FONKSÄ°YONU ---
def cevapla(soru):
    if not soru:
        return ""
    
    try:
        if qa_chain:
            # RAG ile cevapla (Veri + Genel Bilgi)
            ham_cevap = qa_chain.invoke({"query": soru})
            metin = ham_cevap["result"]
        else:
            # Sadece modelin kendi bilgisiyle cevapla
            prompt = f"<|user|>\n{soru}\n</s>\n<|assistant|>\n"
            metin = pipe(prompt)[0]['generated_text']

        # Temizlik
        if "<|assistant|>" in metin:
            temiz_cevap = metin.split("<|assistant|>")[-1]
        else:
            temiz_cevap = metin

        return temiz_cevap.strip()
        
    except Exception as e:
        return f"Hata oluÅŸtu: {str(e)}"

# --- 6. ARAYÃœZ ---
arayuz = gr.Interface(
    fn=cevapla,
    inputs=gr.Textbox(lines=2, placeholder="Ã–rn: Alzheimer hastalarÄ± araba kullanabilir mi?"),
    outputs=gr.Textbox(label="Uzman CevabÄ±"),
    title="ğŸ§  Alzheimer Uzman AsistanÄ± (GeniÅŸ KapsamlÄ±)",
    description="Hem yÃ¼klenen verileri hem de genel tÄ±bbi bilgiyi kullanarak cevap verir."
)

if __name__ == "__main__":
    arayuz.launch(inbrowser=True)
