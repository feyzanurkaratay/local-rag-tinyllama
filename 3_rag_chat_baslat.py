import torch
from transformers import pipeline
from langchain_community.llms import HuggingFacePipeline
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS
from langchain.chains import RetrievalQA
from langchain.prompts import PromptTemplate
import warnings
import sys

# UyarÄ±larÄ± gizle
warnings.filterwarnings("ignore")

def chat_baslat():
    print("ğŸš€ TinyLlama RAG AsistanÄ± baÅŸlatÄ±lÄ±yor... (DÃ¼zeltilmiÅŸ Versiyon)")

    # 1. BEYÄ°N (TinyLlama)
    print("ğŸ§  Model yÃ¼kleniyor...")
    model_id = "TinyLlama/TinyLlama-1.1B-Chat-v1.0"

    pipe = pipeline(
        "text-generation",
        model=model_id,
        torch_dtype=torch.float32, 
        device_map="auto",
        max_new_tokens=256,
        do_sample=True,
        temperature=0.2,    # Daha tutarlÄ± olmasÄ± iÃ§in dÃ¼ÅŸÃ¼rdÃ¼k
        top_p=0.95,
        repetition_penalty=1.15  # <--- Ä°ÅTE SÄ°HÄ°RLÄ° AYAR! (Tekrar etmeyi engeller)
    )
    llm = HuggingFacePipeline(pipeline=pipe)

    # 2. HAFIZA
    print("ğŸ“š HafÄ±za yÃ¼kleniyor...")
    embedding_model = HuggingFaceEmbeddings(model_name="sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2")
    
    try:
        # GÃ¼venlik uyarÄ±sÄ±nÄ± aÅŸmak iÃ§in allow_dangerous_deserialization=True
        vector_store = FAISS.load_local("faiss_index_alzheimer_tr", embedding_model, allow_dangerous_deserialization=True)
    except:
        # Eski versiyonlar iÃ§in yedek
        vector_store = FAISS.load_local("faiss_index_alzheimer_tr", embedding_model)

    # 3. KURAL (PROMPT) - TinyLlama'nÄ±n Kendi Ã–zel FormatÄ±
    # Bu format modelin nerede durmasÄ± gerektiÄŸini netleÅŸtirir.
    template = """<|system|>
Sen yardÄ±mcÄ± bir asistansÄ±n. AÅŸaÄŸÄ±daki baÄŸlamÄ± (Context) kullanarak soruyu cevapla.
CevabÄ± verdikten sonra dur. Sadece TÃœRKÃ‡E konuÅŸ.

BaÄŸlam:
{context}
</s>
<|user|>
{question}
</s>
<|assistant|>
"""

    PROMPT = PromptTemplate(template=template, input_variables=["context", "question"])

    # 4. ZÄ°NCÄ°R
    qa_chain = RetrievalQA.from_chain_type(
        llm=llm,
        chain_type="stuff",
        retriever=vector_store.as_retriever(search_kwargs={"k": 3}),
        chain_type_kwargs={"prompt": PROMPT}
    )

    print("\n" + "*"*50)
    print("ğŸ¤– ASÄ°STAN HAZIR! (Ã‡Ä±kmak iÃ§in 'q' yazÄ±n)")
    print("*"*50)

    while True:
        try:
            soru = input("\nğŸ¤” Sorunuz: ")
            if soru.lower() in ["q", "Ã§Ä±kÄ±ÅŸ", "exit"]:
                print("ğŸ‘‹ GÃ¶rÃ¼ÅŸmek Ã¼zere!")
                break
            if not soru.strip():
                continue
            
            print("... YanÄ±t hazÄ±rlanÄ±yor ...")
            # invoke yerine __call__ veya run kullanarak eski versiyon uyumluluÄŸunu artÄ±ralÄ±m
            sonuc = qa_chain.invoke({"query": soru})
            
            print("-" * 40)
            # CevabÄ±n sadece ilgili kÄ±smÄ±nÄ± alÄ±p temizleyelim
            cevap = sonuc['result']
            
            # EÄŸer model yine de saÃ§malarsa temizlemek iÃ§in ek gÃ¼venlik:
            if "<|assistant|>" in cevap:
                cevap = cevap.split("<|assistant|>")[-1]
            
            print(f"ğŸ—£ï¸  CEVAP: {cevap.strip()}")
            print("-" * 40)
            
        except KeyboardInterrupt:
            break
        except Exception as e:
            print(f"âš ï¸ Hata: {e}")

if __name__ == "__main__":
    chat_baslat()
