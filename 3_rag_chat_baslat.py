import torch
from transformers import pipeline
from langchain_community.llms import HuggingFacePipeline
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS
from langchain.chains import RetrievalQA
from langchain.prompts import PromptTemplate
from langchain_community.document_loaders import TextLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
import warnings
import sys
import os

# UyarÄ±larÄ± gizle
warnings.filterwarnings("ignore")

def chat_baslat():
    print("ğŸš€ MasaÃ¼stÃ¼ AsistanÄ± BaÅŸlatÄ±lÄ±yor... (Keskin NiÅŸancÄ± Modu)")

    # 1. BEYÄ°N (TinyLlama)
    print("ğŸ§  Model yÃ¼kleniyor...")
    model_id = "TinyLlama/TinyLlama-1.1B-Chat-v1.0"

    pipe = pipeline(
        "text-generation",
        model=model_id,
        # Mac iÃ§in float32 (Windows ise bfloat16 denenebilir ama float32 garantidir)
        torch_dtype=torch.float32, 
        device_map="auto",
        max_new_tokens=256,
        do_sample=True,
        temperature=0.1,         # YaratÄ±cÄ±lÄ±k kapalÄ± (Ciddiyet modu)
        top_p=0.90,
        repetition_penalty=1.2   # PapaÄŸan modunu engelle
    )
    llm = HuggingFacePipeline(pipeline=pipe)

    # 2. HAFIZA
    print("ğŸ“š HafÄ±za yÃ¼kleniyor...")
    embedding_model = HuggingFaceEmbeddings(model_name="sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2")
    
    # HafÄ±za klasÃ¶rÃ¼nÃ¼ kontrol et
    if not os.path.exists("faiss_index_alzheimer_tr"):
        print("âŒ HATA: HafÄ±za bulunamadÄ±! Ã–nce '2_veritabani_olustur.py' Ã§alÄ±ÅŸtÄ±rÄ±n.")
        return

    try:
        vector_store = FAISS.load_local("faiss_index_alzheimer_tr", embedding_model, allow_dangerous_deserialization=True)
    except:
        vector_store = FAISS.load_local("faiss_index_alzheimer_tr", embedding_model)

    # 3. KATI PROMPT (YÃ–NERGE)
    template = """<|system|>
Sen uzman bir Alzheimer asistanÄ±sÄ±n. SANA VERÄ°LEN BAÄLAMI TEKRAR ETME.
AÅŸaÄŸÄ±daki bilgiyi analiz et ve soruya kÄ±sa, net bir TÃ¼rkÃ§e cevap ver.
CevabÄ± verdikten sonra hemen sus.

Bilgi (BaÄŸlam):
{context}
</s>
<|user|>
Soru: {question}
</s>
<|assistant|>
"""

    PROMPT = PromptTemplate(template=template, input_variables=["context", "question"])

    # 4. ZÄ°NCÄ°R
    qa_chain = RetrievalQA.from_chain_type(
        llm=llm,
        chain_type="stuff",
        retriever=vector_store.as_retriever(search_kwargs={"k": 2}), # Sadece en alakalÄ± 2 parÃ§a
        chain_type_kwargs={"prompt": PROMPT}
    )

    print("\n" + "*"*50)
    print("ğŸ¤– UZMAN ASÄ°STAN HAZIR! (Ã‡Ä±kmak iÃ§in 'q' yazÄ±n)")
    print("*"*50)

    # 5. SOHBET DÃ–NGÃœSÃœ
    while True:
        try:
            soru = input("\nğŸ¤” Sorunuz: ")
            if soru.lower() in ["q", "Ã§Ä±kÄ±ÅŸ", "exit"]:
                print("ğŸ‘‹ GÃ¶rÃ¼ÅŸmek Ã¼zere!")
                break
            if not soru.strip():
                continue
            
            print("... Analiz ediliyor ...")
            
            # CevabÄ± al
            ham_cevap = qa_chain.invoke({"query": soru})
            metin = ham_cevap['result']

            # --- TEMÄ°ZLÄ°K ROBOTU ---
            # CevabÄ±n iÃ§indeki teknik etiketleri ve tekrarlarÄ± temizle
            if "<|assistant|>" in metin:
                temiz_cevap = metin.split("<|assistant|>")[-1]
            else:
                temiz_cevap = metin
            
            if "BaÄŸlam:" in temiz_cevap:
                temiz_cevap = temiz_cevap.split("BaÄŸlam:")[0]

            print("-" * 40)
            print(f"ğŸ—£ï¸  CEVAP: {temiz_cevap.strip()}")
            print("-" * 40)
            
        except KeyboardInterrupt:
            break
        except Exception as e:
            print(f"âš ï¸ Hata: {e}")

if __name__ == "__main__":
    chat_baslat()
